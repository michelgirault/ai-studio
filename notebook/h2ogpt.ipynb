{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check env variables\n",
    "!env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone the main repository of h2o\n",
    "!git clone https://github.com/h2oai/h2ogpt.git apps/h2ogpt/\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create local env\n",
    "!python3 -m venv apps/h2ogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the git repository \n",
    "%%bash\n",
    "source h2ogpt/bin/activate\n",
    "cd h2ogpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install requiements lib\n",
    "%pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies and lib for h20gpt\n",
    "%pip install -r requirements.txt\n",
    "%pip install -r reqs_optional/requirements_optional_langchain.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pack of additional inforamtio\n",
    "%pip uninstall llama_cpp_python llama_cpp_python_cuda -y\n",
    "%pip install -r reqs_optional/requirements_optional_llamacpp_gpt4all.txt --no-cache-dir\n",
    "%pip install -r reqs_optional/requirements_optional_langchain.urls.txt\n",
    "%pip install -r reqs_optional/requirements_optional_langchain.urls.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the model selected\n",
    "#python3 generate.py --base_model=TheBloke/Mistral-7B-Instruct-v0.2-GGUF --prompt_type=mistral --max_seq_len=4096 --share=True \n",
    "!python3 generate.py --base_model=$MODEL_LOCALPATH/Llama-2-7B.gguf --prompt_type=llama2 --openai_server=True --share=True "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
